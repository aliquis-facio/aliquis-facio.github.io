**기계 번역(Neural Machine Translation)**, **텍스트 생성(GPT류)**, **음성 인식**, **이미지 캡션 생성** 등 “**시퀀스 생성(sequence generation)**” 문제의 _출력 단계(Decoder)_ 에서 핵심적으로 사용되는 알고리즘

---

## 1. 기본 정의

**Beam Search Decoder**: “문장을 생성할 때, 매 단계마다 가장 가능성 높은 몇 가지 후보(beam)를 유지하며 최종적으로 가장 확률이 높은 시퀀스를 선택하는 탐색 알고리즘”이다.

**가장 확률이 높은 문장 하나를 찾기 위한 “현실적인 근사 탐색법”** 

---

## 2. 왜 필요한가?

문장 생성 모델(예: Seq2Seq, LSTM, Transformer)은 다음 단어의 확률 분포를 예측한다.

$$P(y_1, y_2, ..., y_T | x) = \prod_{t=1}^T P(y_t | y_1, ..., y_{t-1}, x)$$

**완전 탐색(Exhaustive Search)** 의 시간적 비용이 너무 큼
따라서 현실적으로, **가장 가능성이 높은 몇 개의 후보만 유지**하면서 탐색하는 방법이 필요 → **Beam Search**

---

## 3️. 작동 원리

1. 디코더가 첫 번째 단어 확률을 예측
    예) “I (0.5)”, “You (0.3)”, “He (0.2)”
2. **상위 Beam Size(k)** 개 선택
	예: k = 2 → [“I” (0.5), “You” (0.3)]
3. 각 후보(“I”, “You”)에 대해 다음 단어 확률 계산
    ```
    “I” → “am”(0.4), “was”(0.3), “like”(0.2)
    “You” → “are”(0.6), “were”(0.2), “go”(0.1)
    ```
4. 조합별 확률 계산 (이전 확률 × 현재 확률)
    ```
    I am (0.5×0.4 = 0.20)
    I was (0.5×0.3 = 0.15)
    You are (0.3×0.6 = 0.18)
    You were (0.3×0.2 = 0.06)
    ```
5. 상위 2개 선택:
	[I am (0.20), You are (0.18)]
6. 다음 단어 예측 반복
7. 매 단계마다 **가장 확률이 높은 k개의 시퀀스**만 남김
8. 모든 후보가 “”(종료 토큰)에 도달하면 종료
9. 그중 **가장 높은 전체 확률**의 시퀀스가 최종 출력

---

## 4. 수식 표현

$$\text{Score}(Y_{1:t}) = \prod_{i=1}^{t} P(y_i | y_{<i}, x)$$

혹은 로그를 사용하여 수치 안정화:
$$\text{Score}(Y_{1:t}) = \sum_{i=1}^{t} \log P(y_i | y_{<i}, x)$$
- 실제 구현에서는 log-sum을 사용해 underflow 방지
- 길이가 긴 문장이 불리하므로 **길이 보정(length normalization)** 적용하기도 함

---

## 5️. “Beam Size (빔 크기)”의 역할

|Beam Size|설명|결과|
|---|---|---|
|**1 (Greedy Search)**|항상 가장 확률 높은 단어만 선택|빠르지만 전역 최적 해 탐색 어려움|
|**3~5**|상위 후보 몇 개를 유지|품질과 속도 균형|
|**∞ (Exhaustive Search)**|모든 경우 탐색|정확하지만 계산량 폭발|

Beam Size ↑, 더 좋은 문장을 찾을 확률 ↑, 계산량 ↑

---
## 6. 예시

|단계|후보 문장 (beam size=2)|누적 확률|
|---|---|---|
|Step 1|“I”(0.5), “You”(0.3)|0.5, 0.3|
|Step 2|“I am”(0.20), “You are”(0.18)|0.20, 0.18|
|Step 3|“I am happy”(0.12), “You are smart”(0.09)|0.12, 0.09|
|Step 4|종료() → “I am happy” 선택|✅|

---

## 7. Beam Search vs Greedy Search 비교

|구분|Greedy Search|Beam Search|
|---|---|---|
|**전략**|매 단계 최선 단어 선택|상위 k개 후보 유지|
|**속도**|빠름|조금 느림|
|**결과 품질**|종종 최적이 아님|더 나은 결과 가능|
|**탐색 다양성**|낮음|높음|
|**사용 예시**|단순 예측|번역, 요약, 생성 모델|

---

## 8. 변형 기법들

|알고리즘|특징|
|---|---|
|**Diverse Beam Search**|후보들 간 유사도 줄여 더 다양한 결과|
|**Length Normalized Beam Search**|긴 문장 불이익 방지|
|**Temperature Sampling / Top-k Sampling**|확률 분포 기반 확률적 선택 (GPT 계열)|
