# SGD (Stochastic Gradient Descent, 확률적 경사 하강법)

## 1. 기본 개념

**SGD(Stochastic Gradient Descent, 확률적 경사 하강법)**: **손실 함수(loss function)** 를 최소화하기 위해 **가중치(파라미터)** 를 점진적으로 업데이트하는 **최적화 알고리즘**

모델의 출력이 정답과 얼마나 다른지를 계산하고, 그 오차를 줄이는 방향으로 가중치를 조금씩 조정하는 방법

---

## 2. 수식 표현

손실 함수 $L(\theta)$ 에 대해, 매개변수(가중치) $\theta$를 업데이트하는 규칙은 다음과 같다:
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)$$

| 기호                          | 의미                                   |
| --------------------------- | ------------------------------------ |
| $\theta_t$                  | t번째 학습 단계의 파라미터(가중치)                 |
| $\eta$                      | 학습률(learning rate): 한 번에 얼마나 이동할지 결정 |
| $\nabla_\theta L(\theta_t)$ | 손실 함수의 경사(gradient)                  |
| $\theta_{t+1}$              | 업데이트된 파라미터                           |

경사가 큰 방향은 손실이 증가하므로, 그 반대 방향(–gradient)으로 이동한다.

---

## 3. “Stochastic”의 의미

- **“Stochastic”**: **확률적**, 즉 “무작위적”
- 전체 데이터셋을 한 번에 쓰지 않고, **작은 무작위 샘플(mini-batch)** 로 나누어 학습한다.

| 종류                                    | 설명                         | 장단점         |
| ------------------------------------- | -------------------------- | ----------- |
| **Batch Gradient Descent**            | 전체 데이터를 한 번에 학습            | 정확하지만 매우 느림 |
| **Stochastic Gradient Descent (SGD)** | 1개 샘플씩 업데이트                | 빠르지만 잡음이 큼  |
| **Mini-batch SGD**                    | 여러 샘플(예: 32, 64개)로 평균 업데이트 | 속도와 안정성의 절충 |

대부분의 딥러닝 프레임워크는 사실상 **“Mini-batch SGD”** 방식을 사용한다.

---

## 4. SGD의 학습 과정

1. **Forward Pass**: 입력을 모델에 넣고 예측값 계산
2. **Loss 계산**: 예측값과 실제값의 차이로 손실(loss) 계산
3. **Backward Pass (역전파)**: 미분을 통해 각 파라미터의 gradient 계산
4. **Update (업데이트)**: $\theta_{new} = \theta_{old} - \eta \times \text{gradient}$

이 과정을 **매번 데이터 배치마다 반복**한다.

---

## 5. SGD의 주요 하이퍼파라미터

|항목|설명|
|---|---|
|**Learning Rate (η)**|얼마나 크게 이동할지 결정. 너무 크면 발산, 너무 작으면 느림|
|**Batch Size**|한 번에 학습하는 샘플 개수. 작을수록 노이즈 큼|
|**Epoch**|전체 데이터셋을 몇 번 반복 학습할지|

---

## 6. SGD의 개선 알고리즘들

|알고리즘|특징|
|---|---|
|**Momentum**|이전 gradient의 관성(momentum)을 반영해 더 부드럽게 이동|
|**Nesterov Momentum**|이동 후 gradient 예측 (조금 더 빠르고 안정적)|
|**Adagrad**|학습률을 파라미터별로 자동 조정|
|**RMSProp**|과거 gradient의 제곱 평균으로 학습률 안정화|
|**Adam**|Momentum + RMSProp 결합 (가장 널리 쓰이는 방법)|

대부분의 딥러닝 프레임워크(`torch.optim.Adam`, `tf.keras.optimizers.Adam`)도 결국 **SGD의 확장 버전**이다.

---

## 7. SGD의 장점과 단점

|구분|장점|단점|
|---|---|---|
|**장점**|단순하고 구현 쉬움, 대규모 데이터에 효율적, 빠른 업데이트|매우 불안정할 수 있음 (노이즈), 수렴이 느릴 수 있음|
|**해결책**|→ Momentum, Adam, 학습률 스케줄링 등 사용||
