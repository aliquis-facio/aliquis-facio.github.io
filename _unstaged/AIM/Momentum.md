# Momentum
## 1. 기본 정의

**Momentum(모멘텀)**: **경사 하강법(Gradient Descent)** 에 **“관성(Inertia)”** 을 추가한 최적화 기법이다.

“현재의 변화량(gradient)”뿐 아니라 **이전 단계의 이동 방향**을 일정 비율로 유지하여 **더 빠르고 안정적으로 최솟값에 도달하도록 돕는 방법**

---

## 2. 왜 필요한가? — 기본 경사 하강법의 한계

기본 **SGD** 업데이트 식은 다음과 같다:
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)$$

| 기호                          | 의미                 |
| --------------------------- | ------------------ |
| $\theta_t$                  | 현재 파라미터 (가중치)      |
| $\eta$                      | 학습률(learning rate) |
| $\nabla_\theta L(\theta_t)$ | 현재 손실함수의 기울기       |

이 방식은 단순하지만,
- **곡면(curved surface)** 에서 **“지그재그(zigzag)”** 로 흔들리며 느리게 수렴
- **좁고 가파른 골짜기(loss valley)** 에서 오락가락
- 지역 최솟값(local minima) 근처에서 멈춤 가능성

→ 이걸 부드럽게 만들고, 안정적이고 빠르게 수렴시키는 게 **Momentum의 역할**

---

## 3. Momentum의 핵심 아이디어

> “이전 단계에서 이동하던 방향으로 관성을 조금 유지하자.”

이전 단계의 이동 속도를 기억하고, 이번 단계에서도 일부를 반영

---

## 4. 수식

$$v_t = \beta v_{t-1} + (1 - \beta)\nabla_\theta L(\theta_t)$$
$$\theta_{t+1} = \theta_t - \eta v_t$$

| 기호                          | 의미                               |
| --------------------------- | -------------------------------- |
| $v_t$                       | “속도(velocity)” – 이전 단계의 이동 방향 누적 |
| $\beta$                     | 모멘텀 계수 (보통 0.9 정도)               |
| $\eta$                      | 학습률                              |
| $\nabla_\theta L(\theta_t)$ | 현재 기울기                           |

- $\beta v_{t-1}$: 이전 방향을 **몇 퍼센트 유지**할지 (관성)
- $(1-\beta)\nabla_\theta L(\theta_t)$: 새로 계산된 기울기를 **얼마나 반영할지**

이전 이동 방향과 현재 gradient를 **평균 내듯 섞어서** 더 부드러운 업데이트를 만든다.

---

## 5. 하이퍼파라미터 — β (베타)

|β 값|의미|효과|
|---|---|---|
|0|Momentum 없음 (일반 SGD)|느리고 불안정|
|0.5|과거 정보 절반 반영|약간의 안정화|
|**0.9**|일반적으로 사용|빠르고 안정적 수렴|
|0.99↑|너무 크면 오래된 정보에 끌려 느려짐|과적응 위험|

대부분의 딥러닝 프레임워크(예: TensorFlow, PyTorch)는 기본값 β = **0.9** 를 사용한다.

---

## 6. Momentum의 변형들

|알고리즘|특징|
|---|---|
|**Nesterov Momentum (NAG)**|미리 한 스텝 이동 후 gradient 계산 (더 정교한 방향 예측)|
|**RMSProp**|gradient 크기를 제곱 평균으로 정규화 (adaptive learning rate)|
|**Adam**|Momentum + RMSProp 결합 (현재 표준)|
**Adam = “Momentum + Adaptive Learning Rate”**