# Sequence

“시간적 또는 순서적 의미가 있는 데이터의 연속”

## 1. 기본 정의

> **Sequence(시퀀스)** = “순서가 있는 데이터의 나열”

즉, 단순히 여러 개의 데이터가 모여 있는 집합(set)이 아니라, **“순서(order)” 자체가 의미를 가지는 구조**예요.

예를 들어:
- 영어 문장: “I love pizza.” → 단어 순서가 중요 (love I pizza ❌)
- 주식 시계열: 오늘 가격 → 내일 가격 → 모레 가격
- 오디오 신호: 샘플이 시간 순으로 이어진 파형

이처럼 “이전 정보가 다음 정보에 영향을 주는” 데이터의 형태를 **sequence**라고 합니다.

## 2. 시퀀스의 종류별 예시

|종류|예시|특징|
|---|---|---|
|**텍스트(Text)**|단어 또는 문자 시퀀스 — “I love you”|문맥(context) 순서 중요|
|**음성(Speech)**|오디오 파형 시퀀스|시간적 연속성 유지|
|**비디오(Video)**|프레임 시퀀스|앞뒤 프레임 간 관계 중요|
|**시계열(Time-series)**|주가, 기온, 센서 데이터 등|시간 순서가 핵심|
|**이벤트 로그(Event sequence)**|클릭 로그, 행동 순서 등|순서에 따라 의미 달라짐|

## 3. 수학적 표현

시퀀스는 보통 다음처럼 표현합니다:

$$x_1, x_2, x_3, \ldots, x_T  $$

- $x_t$: 시퀀스의 t번째 원소 (단어, 숫자, 신호 등)
- $T$: 전체 시퀀스 길이

예: 문장 “I love pizza” →  
$$x_1 = \text{I},\quad x_2 = \text{love},\quad x_3 = \text{pizza}$$

## 4. 시퀀스를 다루는 주요 모델들

|모델|시퀀스를 다루는 방식|
|---|---|
|**RNN (Recurrent Neural Network)**|순차적으로 입력을 처리하며, 과거 정보를 “은닉 상태(hidden state)”에 저장|
|**LSTM / GRU**|RNN의 확장 — 긴 시퀀스에서 장기 의존성(long-term dependency) 문제 해결|
|**Transformer**|모든 시점의 단어를 동시에 보고, **Attention**으로 문맥 관계를 학습|
|**Seq2Seq (Sequence-to-Sequence)**|한 시퀀스(입력)를 다른 시퀀스(출력)로 변환 — 예: 번역, 요약 등|

## 5. 예시로 이해하기

|도메인|입력 시퀀스|출력 시퀀스|태스크|
|---|---|---|---|
|**기계 번역 (MT)**|“I love you”|“Je t’aime”|영어 → 프랑스어|
|**음성 인식 (ASR)**|음성 파형|“hello world”|오디오 → 텍스트|
|**텍스트 요약 (Summarization)**|뉴스 기사|제목|긴 문장 → 짧은 문장|
|**챗봇 대화 (Dialogue)**|질문|답변|문장 → 문장|

## 6. 시퀀스의 핵심 특징 요약

|특징|설명|
|---|---|
|**순서성(Order)**|데이터의 순서가 의미를 결정함|
|**의존성(Dependency)**|이전 요소가 이후 요소에 영향을 줌|
|**가변 길이(Variable Length)**|시퀀스의 길이는 일정하지 않음|
|**시간성(Temporal nature)**|시점에 따라 의미가 달라짐|
|**문맥성(Context)**|전체 시퀀스의 맥락이 중요|
