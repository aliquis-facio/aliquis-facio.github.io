# RNN(Recurrent Neural Network, 순환 신경망)

RNN은 인공지능, 특히 **자연어 처리(NLP)** 와 **시계열 분석(time-series)** 의 기본 골격이 되는 신경망이다.

---

## 1. 기본 정의

**RNN (Recurrent Neural Network)**: **이전 시점의 출력 정보를 다음 시점의 입력으로 다시 사용하는 신경망 구조**이다.

입력 데이터가 **순서(sequence)** 를 가진 경우 (문장, 음성, 주가 등), 이전 단계의 정보를 “기억”하여 현재의 출력을 결정한다.

RNN은 “**순서를 이해하는 신경망**”이다.

---

## 2. 핵심 아이디어

일반적인 인공신경망(Feedforward NN)은 `입력 → 은닉층 → 출력` 한 방향으로만 흐르지만, **RNN은 피드백 루프(recurrent connection)** 을 가진다.

```
        ┌──────────┐
x₁ → h₁ ──► y₁
     ▲     │
     │     ▼
     └──── h₂ ──► y₂
           │
           ▼
          ...
```

즉, **이전 단계(hidden state)** 의 결과를 **다음 단계**의 입력과 함께 다시 사용하여 시점 간의 **의존성(temporal dependency)** 을 모델링한다.

---

## 3. RNN의 수식 표현

RNN의 동작을 수식으로 나타내면 다음과 같습니다:
* $$h_t = f(W_x x_t + W_h h_{t-1} + b_h)$$
* $$y_t = g(W_y h_t + b_y)$$

| 기호              | 의미                                |
| --------------- | --------------------------------- |
| $x_t$           | t번째 입력 (예: 문장의 t번째 단어)            |
| $h_t$           | t번째 은닉 상태 (hidden state)          |
| $y_t$           | t번째 출력 (예: 다음 단어 예측 결과)           |
| $W_x, W_h, W_y$ | 학습되는 가중치 행렬                       |
| $f, g$          | 비선형 함수 (보통 tanh, ReLU, softmax 등) |

$h_t$가 $h_{t-1}$ 을 포함한다. 즉, 이전의 정보를 현재 상태로 “전달(remember)”할 수 있다.

---

## 4. RNN의 작동 과정 (문장 예시)

1. 문장:
```
나는 학교에 간다.
```
2. 시퀀스 입력:

| 시점  | 입력 단어 ((x_t)) | 은닉 상태 ((h_t))         | 출력 ((y_t)) |
| --- | ------------- | --------------------- | ---------- |
| t=1 | 나는            | h₁ = f(Wₓ·x₁)         | y₁         |
| t=2 | 학교에           | h₂ = f(Wₓ·x₂ + Wₕ·h₁) | y₂         |
| t=3 | 간다            | h₃ = f(Wₓ·x₃ + Wₕ·h₂) | y₃         |

즉, **이전 단어의 의미(문맥)** 가 다음 단어의 해석에 영향을 준다. “나는” → “학교에” → “간다” 로 자연스럽게 연결된다.

---

## 5. RNN의 구조적 특징

|항목|설명|
|---|---|
|**파라미터 공유(Parameter sharing)**|모든 시점에서 동일한 가중치 (W_x, W_h, W_y) 사용 → 일반화 용이|
|**시간에 따른 전개 (Unfolding through time)**|여러 시점을 펼쳐서 학습 (Backpropagation Through Time, BPTT)|
|**메모리 효과 (Temporal memory)**|이전 정보가 은닉 상태로 저장되어 시점 간 의존성 학습 가능|

---

## 6. RNN의 한계 — 왜 [LSTM](AIM/Long_Short_Term_Memory.md)이 필요한가?

|문제|설명|
|---|---|
|**기울기 소실 (Vanishing Gradient)**|긴 시퀀스일수록 과거 정보의 영향이 사라짐|
|**기울기 폭발 (Exploding Gradient)**|반대로, 기울기가 너무 커져 학습 불안정|
|**장기 의존성 학습 어려움**|“10단어 전” 정보는 거의 반영되지 않음|
|**병렬화 어려움**|시퀀스 특성상 계산이 순차적으로 진행되어 속도 느림|

이 문제를 해결하기 위해 **LSTM** 과 **GRU** 가 등장했습니다. (LSTM은 “정보를 얼마나 잊고, 얼마나 유지할지”를 제어하는 게이트 구조를 추가함)

---

## 7. RNN의 변형 모델들

|모델|특징|
|---|---|
|**LSTM (Long Short-Term Memory)**|장기 의존성 문제 해결 (기억/망각 게이트)|
|**GRU (Gated Recurrent Unit)**|LSTM보다 간단하지만 유사한 성능|
|**Bi-RNN (Bidirectional RNN)**|앞뒤 문맥 모두 고려|
|**Seq2Seq (Encoder–Decoder)**|입력 시퀀스를 벡터로 요약 후 새로운 시퀀스로 디코딩|
|**Attention RNN**|중요한 부분에 가중치 부여 (Transformer의 전신)|

---

## 8. RNN의 주요 응용 분야

|분야|활용 예시|
|---|---|
|**자연어 처리 (NLP)**|기계 번역, 텍스트 생성, 감정 분석|
|**음성 인식 (ASR)**|음성 → 텍스트 변환|
|**시계열 분석**|주가 예측, 센서 데이터 분석|
|**영상 처리**|비디오의 시간적 변화 감지|
|**대화형 AI**|문맥 기반 답변 생성|
