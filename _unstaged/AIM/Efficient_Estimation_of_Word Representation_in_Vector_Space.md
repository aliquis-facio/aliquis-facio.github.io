# 단어 벡터 공간 표현의 효율적 추정

**저자**
- Tomas Mikolov (Google Inc., Mountain View, CA)
- Kai Chen (Google Inc., Mountain View, CA)
- Greg Corrado (Google Inc., Mountain View, CA)
- Jeffrey Dean (Google Inc., Mountain View, CA)
    
---

## 초록 (Abstract)

우리는 매우 큰 데이터셋으로부터 단어의 연속적인 벡터 표현을 계산하기 위한 두 가지 새로운 모델 아키텍처를 제안한다. 이 표현의 품질은 단어 유사성(word similarity) 과제를 통해 측정되며, 이전의 다양한 신경망 기반 기법과 비교된다. 우리의 방법은 훨씬 낮은 계산 비용으로 정확도에서 큰 개선을 보인다. 예를 들어, 16억 개 단어 데이터셋에서 하루도 안 되는 시간 안에 고품질의 단어 벡터를 학습할 수 있다. 또한, 이 벡터들이 문법적(syntactic) 및 의미적(semantic) 단어 유사성 평가에서 최신(state-of-the-art) 성능을 제공함을 보인다.

---

## 1. 서론 (Introduction)

현재 대부분의 자연어처리(NLP) 시스템은 단어를 **원자적 단위(atomic unit)로 취급**한다. 즉, **단어 간 유사성은 고려하지 않고**, 단어는 단순히 어휘집(vocabulary) 내의 인덱스로 표현된다. 이는 단순성, 견고성, 그리고 "많은 데이터에서 단순한 모델이 적은 데이터에서의 복잡한 모델보다 낫다"는 경험적 사실 덕분이다. 대표적인 예로 **N-gram 언어모델**이 있으며, 이는 오늘날 거의 모든 이용 가능한 데이터(수 조 단어)에서 학습될 수 있다.

그러나 이러한 단순 기법은 이미 한계에 도달했다. 예를 들어, **자동 음성 인식(ASR)** 에서는 고품질 전사 데이터가 수백만 단어 수준에 불과해 데이터 부족 문제가 발생한다. **기계 번역(MT)** 에서도 다수의 언어 쌍에 대해 수십억 단어 이하의 코퍼스만 존재한다. 따라서 단순한 데이터 확장은 충분하지 않고, 더 정교한 기법이 필요하다.

최근 머신러닝의 발전으로 더 복잡한 모델이 훨씬 큰 데이터셋에서 학습될 수 있게 되었으며, 이는 단순 모델보다 성능이 우수하다. 특히 **단어의 분산 표현(distributed representations)** 개념이 주목받고 있다. 예를 들어, **신경망 기반 언어 모델(Neural Network Language Model, NNLM)** 은 전통적 N-gram 모델보다 훨씬 뛰어난 성능을 보인다.
<font color="#92d050">- 기존 NLP 시스템에서는 단어 간 유사성 고려 X e.g. N-gram</font>
<font color="#92d050">- 단어의 분산 표현 고려해보자!</font>

---

### 1.1 본 논문의 목표

이 논문의 주요 목표는 수십억 단어와 수백만 개의 어휘를 포함하는 방대한 데이터셋으로부터 **고품질 단어 벡터(word vectors)** 를 학습할 수 있는 기법을 소개하는 것이다. 우리가 알기로, 지금까지 제안된 어떤 아키텍처도 수억 단어 이상, 그리고 벡터 차원이 50~100 정도의 비교적 낮은 수준을 넘어서 성공적으로 학습된 사례는 없었다.

우리는 최근 제안된 단어 벡터 표현의 품질을 측정하는 기법을 사용한다. 기대하는 바는 유사한 단어들이 서로 가까이 위치할 뿐 아니라, 단어들이 **여러 차원의 유사성**을 가질 수 있다는 것이다. 이는 굴절어(屈折語, inflectional languages)의 맥락에서 이미 관찰된 바 있다. 예컨대, 명사는 여러 가지 단어 끝(어미)을 가질 수 있는데, 원래 벡터 공간의 **부분 공간(subspace)** 에서 유사한 단어들을 탐색하면 비슷한 어미를 가진 단어들을 찾을 수 있다.

더 놀라운 점은, 단어 표현의 유사성이 단순한 문법적 규칙성을 넘어선다는 것이다. 예를 들어, 단순한 대수적 연산을 단어 벡터에 적용하는 **단어 오프셋(word offset) 기법**에서, `vector("King")−vector("Man")+vector("Woman")` 을 계산하면, 그 결과가 단어 **“Queen”** 의 벡터 표현에 가장 가까운 것으로 나타났다.

이 논문에서는 이러한 **벡터 연산의 정확도**를 최대화하기 위해 단어 간의 선형적 규칙성(linear regularities)을 보존하는 새로운 모델 아키텍처를 개발한다. 또한, 문법적(syntactic) 및 의미적(semantic) 규칙성을 모두 측정할 수 있는 새로운 포괄적 테스트 세트를 설계하고, 이 규칙들이 높은 정확도로 학습될 수 있음을 보여준다. 나아가, 단어 벡터의 차원 수와 학습 데이터의 양에 따라 **학습 시간과 정확도가 어떻게 달라지는지**도 논의한다.
<font color="#92d050">목표: 단어의 의미적 관계를 벡터로 나타내자</font>

---

### 1.2 선행 연구 (Previous Work)

단어를 **연속적인 벡터(continuous vectors)** 로 표현하려는 시도는 오래된 역사 [10, 26, 8]를 가지고 있다. **신경망 언어 모델(Neural Network Language Model, NNLM)** 을 추정하기 위한 매우 인기 있는 아키텍처가 [1]에서 제안되었는데, 이 모델은 **선형 투영 층(linear projection layer)** 과 **비선형 은닉층(non-linear hidden layer)** 을 가진 피드포워드 신경망(feedforward neural network)을 사용하여 단어 벡터 표현과 통계적 언어 모델을 동시에 학습한다. 이후 많은 연구들이 이 접근 방식을 발전시켰다.
<font color="#92d050">NNLM 아키텍처:</font>
<font color="#92d050">- 피드포워드 신경망 사용.</font>
<font color="#92d050">- 선형 투영층 + 비선형 은닉층 → 단어 벡터 학습과 언어 모델링 동시 학습</font>

또 다른 흥미로운 NNLM 아키텍처가 [13, 14]에서 제시되었다. 여기서는 먼저 단일 은닉층(single hidden layer)을 가진 신경망을 통해 단어 벡터를 학습한 후, 그 단어 벡터를 사용하여 NNLM을 훈련한다. 즉, **전체 NNLM을 구성하지 않고도 단어 벡터를 먼저 학습**할 수 있었다. 본 논문에서는 바로 이 아키텍처를 확장하여, 단순한 모델을 사용해 단어 벡터를 학습하는 첫 번째 단계에 초점을 맞춘다.
<font color="#92d050">다른 NNLM 아키텍처:</font>
<font color="#92d050">- 단일 은닉층 신경망으로 단어 벡터 학습 → NNLM 훈련</font>
<font color="#92d050">- 전체 NNLM 구성 X, 단어 벡터 학습 가능</font>
<font color="#92d050">- 본 논문에서는 이 방식, 단어 벡터 학습 단계에 집중</font>

이후 단어 벡터가 다양한 NLP 응용에서 성능을 크게 향상시키고 단순화할 수 있음이 입증되었다 [4, 5, 29]. 단어 벡터의 추정 자체는 여러 다른 모델 아키텍처와 다양한 코퍼라에서 수행되었으며 [4, 29, 23, 19, 9], 그 결과 생성된 일부 단어 벡터들은 향후 연구와 비교를 위해 공개되기도 했다². 그러나 우리가 알기로, [13]에서 제안된 아키텍처를 제외하면, 이러한 대부분의 아키텍처는 학습 비용이 훨씬 더 많이 드는 것으로 알려져 있다. 단, **대각선 가중치 행렬(diagonal weight matrices)** 을 사용하는 특정 버전의 **로그-이중선형(log-bilinear) 모델**은 예외이다 [23].
<font color="#92d050">계산 비용 문제:</font>
<font color="#92d050">- 대부분 기존 아키텍처 학습 비용 ↑</font>

---

## 2. 모델 아키텍처 (Model Architectures)

단어의 연속 표현(continuous representations)을 추정하기 위해 다양한 모델이 제안되었다. 그중에는 잘 알려진 **잠재 의미 분석(Latent Semantic Analysis, LSA)** 과 **잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)** 도 포함된다. 그러나 본 논문에서는 **신경망을 통해 학습된 단어의 분산 표현(distributed representations of words)** 에 집중한다. 이전 연구에서, 신경망 기반 접근법이 단어들 사이의 **선형적 규칙성(linear regularities)** 을 보존하는 데 LSA보다 훨씬 뛰어난 성능을 보임이 입증되었기 때문이다 [20, 31]. 또한 LDA는 대규모 데이터셋에서 계산 비용이 매우 커지는 단점이 있다.
<font color="#92d050">기존 기법과 한계</font>
<font color="#92d050">- 단어의 연속 표현을 위해 LSA, LDA 등 사용</font>
    - <font color="#92d050">LSA: 단어 간 선형적 규칙성 보존에 취약</font>
    - <font color="#92d050">LDA: 대규모 데이터셋 계산 비용 매우 큼</font>
<font color="#92d050">신경망 기반 분산 표현이 더 적합하다!</font>

[18]의 방법과 유사하게, 우리는 서로 다른 모델 아키텍처를 비교하기 위해 먼저 모델의 **계산 복잡도(computational complexity)** 를 정의한다. 여기서 계산 복잡도는 모델을 완전히 학습하기 위해 접근해야 하는 **파라미터(parameter)** 의 수로 정의한다. 이후 우리는 계산 복잡도를 최소화하면서 정확도를 최대화하는 방법을 모색한다.

다음에 설명할 모든 모델에서 학습 복잡도는 다음과 비례한다:
$O=E×T×Q$

여기서,
- **E**는 학습 epoch 수,
- **T**는 학습 데이터셋의 단어 수,
- **Q**는 각 모델 아키텍처별로 정의되는 값이다.

일반적으로 **E = 3 ~ 50**, **T는 최대 10억 단어** 수준으로 선택된다.
모든 모델은 **확률적 경사 하강법(Stochastic Gradient Descent, SGD)** 과 **역전파(Backpropagation)** 를 사용하여 학습된다 [26].
<font color="#92d050">→ 모델 학습 복잡도:</font>
<font color="#92d050">O = E×T×Q</font>

---

### 2.1 피드포워드 신경망 언어 모델 (Feedforward Neural Net Language Model, NNLM)

확률적 피드포워드 신경망 언어 모델은 [1]에서 제안되었다. 이 모델은 **입력층(input layer)**, **투영층(projection layer)**, **은닉층(hidden layer)**, **출력층(output layer)** 으로 구성된다.
- 입력층에서는 어휘 크기를 V라 할 때, 이전의 N개 단어가 **1-of-V 인코딩(one-hot encoding)** 방식으로 표현된다.
- 입력층은 공유 투영 행렬을 통해 차원 $N×D$의 투영층으로 사상된다. 이때 한 시점에 활성화되는 입력은 N개뿐이므로, 투영층 계산은 상대적으로 비용이 적게 든다.

문제는 **투영층과 은닉층 사이의 연산**에서 발생한다. 투영층의 값은 밀집(dense)되어 있으므로, 예를 들어 $N=10$일 때 투영층 크기 P는 500~2000 정도가 되고, 은닉층 크기 H는 보통 500~1000 단위이다. **은닉층은 어휘 전체에 대한 확률 분포를 계산**해야 하므로, 출력층의 차원은 V가 된다. 따라서, 학습 샘플 하나에 대한 계산 복잡도는 다음과 같다:

$Q=N×D+N×D×H+H×V$

여기서 지배적인 항(dominant term)은 $H×V$ `은닉층 크기 × 출력층 차원`이다.

이 문제를 피하기 위해 여러 가지 실용적인 해법들이 제안되었다. 예를 들어, **계층적 소프트맥스(Hierarchical Softmax)** 를 사용하거나 [25, 23, 18], 학습 중 정규화를 피하는 비정규화 모델(un-normalized models)을 사용하는 방식 [4, 9]이 있다. 어휘를 이진 트리로 표현하면, 평가해야 하는 출력 단위 수는 약 $log⁡2(V)$로 줄어든다. 따라서 대부분의 복잡도는 $N×D×H$ 항에서 발생한다.

우리의 모델에서는 어휘를 **허프만 이진 트리(Huffman Binary Tree)** 로 표현하는 **계층적 소프트맥스를 사용**한다. 이전 연구 [16]에서, 단어의 빈도 정보를 활용하는 것이 신경망 언어 모델에서 단어 클래스를 얻는 데 효과적임이 관찰되었다. 허프만 트리는 빈도 높은 단어에 짧은 이진 코드를 할당하기 때문에, 평가해야 할 출력 단위의 수가 줄어든다. 균형 이진 트리의 경우 약 $log⁡2(V)$ 개의 출력을 평가해야 하지만, 허프만 트리 기반 계층적 소프트맥스에서는 약 $log⁡2(Unigram perplexity(V))$ 개만 필요하다.

예를 들어, 어휘 크기가 백만 단어일 경우, 이는 평가 속도가 약 두 배 빨라지는 결과를 가져온다. 물론 신경망 언어 모델의 주요 병목 지점은 $N×D×H$ 항이므로 이 가속이 결정적이지는 않지만, 이후에 제안할 은닉층이 없는 아키텍처에서는 소프트맥스 정규화 효율성에 크게 의존하게 된다.
<font color="#92d050">구조: 입력층 → 투영층 → 은닉층 → 출력층</font>
<font color="#92d050">복잡도: `Q = N×D + N×D×H + H×V`</font>
<font color="#92d050">계산병목: `N×D×H`</font>
<font color="#92d050">해결책: 계층적 소프트맥스, 비정규화 모델 사용</font>

---

### 2.2 순환 신경망 언어 모델 (Recurrent Neural Net Language Model, RNNLM)

순환 신경망 기반 언어 모델(RNNLM)은 피드포워드 NNLM의 몇 가지 한계를 극복하기 위해 제안되었다. 대표적인 한계로는 **맥락 길이(context length, 즉 모델의 차수 N)를 사전에 지정해야 한다는 점** 이 있다. 이와 달리, RNN은 이론적으로 얕은 신경망(shallow neural networks)보다 더 복잡한 패턴을 효율적으로 표현할 수 있다 [15, 2].
<font color="#92d050">→ 맥락 길이 지정 필요 없다</font>

RNN 모델에는 **투영층(projection layer)** 이 존재하지 않고, 입력층(input layer), 은닉층(hidden layer), 출력층(output layer)만 있다. 이 모델의 핵심은 **순환 행렬(recurrent matrix)** 인데, 이는 은닉층을 자기 자신과 연결하여 **시간 지연 연결(time-delayed connections)** 을 형성한다. 이를 통해 RNN은 일종의 단기 메모리(short-term memory)를 갖게 된다. 즉, 과거 입력 정보가 은닉층 상태(state)에 저장되고, 새로운 입력과 이전 은닉층 상태를 바탕으로 현재 은닉층이 갱신되는 방식이다.
<font color="#92d050">→ 순환 행렬을 통해 은닉층에서 시간 지연 연결로 단기 메모리를 형성함</font>

RNN 모델의 학습 샘플 하나에 대한 계산 복잡도는 다음과 같다:
$Q=H×H+H×V$

여기서,
- H는 은닉층의 크기,
- V는 어휘 크기,
- 단어 표현의 차원 D는 은닉층 차원 H와 동일하다.

$H×V$ 항은 **계층적 소프트맥스(hierarchical softmax)** 를 사용하면 $H×log⁡2(V)$까지 줄일 수 있다. 따라서 대부분의 계산 복잡도는 $H×H$ 항에서 발생한다.
<font color="#92d050">계산 복잡도: Q = H×H + H×V</font>

---

### 2.3 신경망의 병렬 학습 (Parallel Training of Neural Networks)

방대한 데이터셋에서 모델을 학습하기 위해, 우리는 여러 모델을 **대규모 분산 프레임워크인 DistBelief [6]** 위에 구현했다. 여기에는 피드포워드 NNLM과 본 논문에서 제안하는 새로운 모델들이 포함된다.

이 프레임워크는 동일한 모델의 여러 **복제본(replicas)** 을 병렬로 실행할 수 있게 해준다. 각 복제본은 중앙 서버(centralized server)를 통해 **모든 파라미터를 공유**하며, 그래디언트 업데이트를 동기화한다.

병렬 학습에서는 **미니배치 비동기 경사 하강법(mini-batch asynchronous gradient descent)** 과 **Adagrad [7]** 이라 불리는 적응형 학습률(adaptive learning rate) 기법을 사용한다.

이러한 구조에서는, 하나의 데이터 센터에서 수백 개 이상의 모델 복제본을 동시에 실행하는 것이 일반적이며, 각 복제본은 여러 개의 CPU 코어를 사용한다.

---

## 3. 새로운 로그-선형 모델 (New Log-linear Models)

이 절에서는 **계산 복잡도를 최소화하면서 분산 단어 표현(distributed word representations)** 을 학습하기 위한 두 가지 새로운 모델 아키텍처를 제안한다.

앞서 살펴본 바에 따르면, 대부분의 계산 복잡도는 모델의 **비선형 은닉층(non-linear hidden layer)** 에서 발생한다. 은닉층은 신경망의 강력한 표현 능력을 제공하지만, 동시에 학습을 매우 비효율적으로 만든다. 따라서 우리는 데이터 표현력이 다소 떨어질 수 있더라도, **훨씬 더 효율적으로 대규모 데이터에 학습할 수 있는 단순한 모델**을 탐구한다.
<font color="#92d050">- 기존 NNLM, RNNLM의 계산 복잡도는 모델의 비선형 은닉층에서 발생</font>
<font color="#92d050">- 은닉층 제거 시, 표현 능력 ↓ but, 훨씬 더 효율적인 대규모 데이터 학습 가능</font>

이번에 제안하는 아키텍처들은 이전 연구 [13, 14]에서 제안된 방법을 직접 확장한 것이다. 그 연구에서는 **신경망 언어 모델(NNLM)** 을 두 단계로 학습할 수 있음을 보였다:
1. 먼저 간단한 모델로 **연속형 단어 벡터(continuous word vectors)** 를 학습한다.
2. 그다음 이 단어 벡터를 기반으로 **N-gram NNLM**을 훈련한다.
후속 연구들에서는 주로 단어 벡터 학습 자체에 집중해 왔는데, 우리는 [13]에서 제안된 방식을 가장 단순하면서도 효과적인 접근으로 본다. 사실, 이와 유사한 모델은 [26, 8] 등 과거 연구에서도 제안된 바 있다.

---

### 3.1 연속형 Bag-of-Words (CBOW) 모델

첫 번째로 제안된 아키텍처는 피드포워드 NNLM과 유사하지만, **비선형 은닉층(non-linear hidden layer)을 제거**하고 **투영층(projection layer)을 모든 단어가 공유**한다(단순히 투영 행렬만 공유하는 것이 아님). 따라서 모든 단어가 동일한 위치로 투영되고, 그들의 벡터는 **평균(average)** 된다.

이 아키텍처를 **Bag-of-Words 모델**이라 부르는 이유는, **문맥 내 단어의 순서가 투영 결과에 영향을 주지 않기 때문**이다. 또한, 우리는 과거 단어뿐만 아니라 **미래 단어도 함께 사용**한다. 구체적으로는, 입력에 과거 단어 4개와 미래 단어 4개를 포함한 **로그-선형 분류기(log-linear classifier)** 를 구축했을 때 가장 좋은 성능을 얻었다. 이때 학습 기준은 현재(가운데) 단어를 올바르게 분류하는 것이다.

학습 복잡도는 다음과 같다:
$Q=N×D+D×log⁡2(V)$

여기서,
- N: 문맥 단어 수 (history + future),
- D: 단어 벡터 차원,
- V: 어휘 크기.

이 모델을 **CBOW(Continuous Bag-of-Words)** 라 부른다. 이는 기존의 단순 BoW 모델과 달리, 문맥(context)을 **연속형 분산 표현(continuous distributed representation)** 으로 사용하기 때문이다. 그림 1에 모델 아키텍처가 제시되어 있다. 참고로, 입력층과 투영층 사이의 가중치 행렬은 NNLM에서와 동일하게 **모든 단어 위치에 대해 공유**된다.
-<font color="#92d050"> 구조: 문맥 단어(과거 + 미래)의 벡터 평균 → 현재 단어 예측</font>
<font color="#92d050">- 특징:</font>
<font color="#92d050">    - 단어 순서 무시</font>
<font color="#92d050">    - history + future context 모두 활용</font>
<font color="#92d050">    - 입력층과 투영층 사이의 가중치 행렬은 모든 단어 위치에서 공유</font>
<font color="#92d050">- 학습 복잡도: `Q = N×D + D×log2(V)`</font>

---

### 3.2 연속형 Skip-gram 모델

두 번째 아키텍처는 CBOW와 유사하지만, **문맥(context)으로 현재 단어를 예측하는 대신, 하나의 단어로부터 같은 문장 내의 다른 단어들을 예측**하려고 한다. 좀 더 구체적으로, 각 현재 단어를 입력으로 하여 **연속형 투영층(continuous projection layer)** 을 가진 로그-선형 분류기(log-linear classifier)에 넣고, 그 단어의 앞뒤 일정 범위 안에 있는 단어들을 예측한다. 실험 결과, **범위(range)를 늘릴수록 학습된 단어 벡터의 품질은 향상**되지만, 동시에 계산 복잡도 역시 증가한다. 또한 일반적으로 **멀리 떨어진 단어일수록 현재 단어와의 관련성이 낮기 때문에**, 학습 시 더 먼 단어는 덜 자주 샘플링하여 가중치를 줄였다.

이 아키텍처의 학습 복잡도는 다음과 비례한다:
$Q=C×(D+D×log⁡2(V))$

여기서,
- C: 고려하는 최대 단어 거리(윈도우 크기).
<font color="#92d050">- 구조: 현재 단어 → 주변 단어들 예측</font>
<font color="#92d050">- 특징:</font>
<font color="#92d050">    - 문맥 단어 개수 범위 C 지정</font>
<font color="#92d050">    - 멀리 떨어진 단어는 덜 샘플링 → 가중치 ↓</font>
<font color="#92d050">- 학습 복잡도: `Q = C×(D + D×log2(V))`</font>

예를 들어 $C=5$라면, 각 학습 단어마다 1에서 $C$ 사이의 값 $R$을 무작위로 선택하고, 현재 단어의 **과거 $R$개 단어와 미래 $R$개 단어**를 정답 레이블로 사용한다. 따라서, 각 학습 샘플마다 현재 단어를 입력으로 두고, 총 $R×2$개의 단어를 출력으로 맞추는 분류 작업을 수행하게 된다.

이후 실험에서는 $C=10$을 사용했다.

![](20250925222709.png)
그림 1에서는 두 아키텍처가 직관적으로 표현된다.
- **CBOW**: 문맥(과거·미래 단어들)으로 현재 단어를 예측.
- **Skip-gram**: 현재 단어로 주변 단어를 예측.

---

## 4. 실험 및 결과 (Results)

서로 다른 버전의 단어 벡터 품질을 비교하기 위해, 기존 연구들은 보통 **예시 단어와 그와 가장 유사한 단어들을 표로 제시**하여 직관적으로 이해하는 방식을 사용한다. 예를 들어, **France가 Italy 및 다른 몇몇 국가들과 유사하다**는 것을 보여주는 것은 쉽지만, 더 복잡한 유사성 과제에 단어 벡터를 적용하는 것은 훨씬 더 도전적이다.
이전 연구 [20]에서는 단어들 사이에 다양한 유형의 유사성이 존재할 수 있음을 지적했다. 예를 들어, **big은 bigger와 유사하고, small은 smaller와 유사**하다는 관계가 있다. 또 다른 관계의 예시는 **big - biggest**와 **small - smallest**라는 단어 쌍이다.
우리는 동일한 관계를 가진 두 단어 쌍을 하나의 **질문(question)** 으로 정의한다. 예를 들어, *“big과 biggest가 그러하듯이, small과 유사한 단어는 무엇인가?”* 라는 질문을 할 수 있다.

놀랍게도, 이러한 질문들은 단어 벡터에 단순한 **대수 연산(algebraic operations)** 을 수행함으로써 답할 수 있다. 예를 들어, _biggest_ 가 _big_ 과 가지는 동일한 관계를 _small_ 과 갖는 단어를 찾기 위해서는, `X=vector("biggest")−vector("big")+vector("small")` 을 계산하면 된다. 그 후, 코사인 거리(cosine distance)를 기준으로 XXX에 가장 가까운 벡터를 탐색하고, 그것을 질문의 답으로 사용한다. (이때 입력된 질문 단어들은 검색에서 제외한다.) 단어 벡터가 잘 학습되었다면, 올바른 답인 **smallest**를 찾을 수 있다.

마지막으로, 우리는 **고차원 단어 벡터(high-dimensional word vectors)** 를 방대한 데이터로 학습했을 때, 단어들 사이의 매우 미묘한 의미적 관계까지 포착할 수 있음을 발견했다. 예를 들어, `France : Paris = Germany : Berlin` 과 같은 **국가와 수도 관계**를 정확히 학습할 수 있었다.
이러한 의미적 관계를 포착하는 단어 벡터는 기계 번역(machine translation), 정보 검색(information retrieval), 질의응답 시스템(question answering systems)과 같은 기존 NLP 응용을 크게 개선할 수 있으며, 아직 발명되지 않은 새로운 응용 가능성도 열어줄 것이다.

![](20250925223452.png)
표 1: **5가지 의미적(semantic) 질문 유형**과 **9가지 문법적(syntactic) 질문 유형**을 포함한 포괄적인 테스트 세트

### 4.1. 작업 묘사
단어 벡터의 품질을 측정하기 위해, 우리는 **5가지 의미적(semantic) 질문 유형**과 **9가지 문법적(syntactic) 질문 유형**을 포함한 포괄적인 테스트 세트를 정의한다. 각 범주에서 두 가지 예시는 표 1에 제시되어 있다. 전체적으로, 의미적 질문은 8,869개, 문법적 질문은 10,675개이다.
각 범주의 질문은 두 단계로 생성되었다.
1. 먼저, 유사한 단어 쌍 목록을 사람이 직접 만든다.
2. 그런 다음, 두 단어 쌍을 연결하여 대규모 질문 집합을 생성한다.
예를 들어, 우리는 **68개의 미국 대도시와 그 도시가 속한 주(state)** 목록을 만들고, 무작위로 두 단어 쌍을 골라 약 2,500개의 질문을 생성했다.
또한, 우리의 테스트 세트에는 **단일 토큰(single token) 단어만 포함**되며, 다중 단어 개체(예: _New York_)는 포함되지 않는다.

우리는 전체 질문 유형에 대한 **전체 정확도(overall accuracy)** 와 각 질문 유형(의미적, 문법적)에 대한 **개별 정확도**를 측정한다.
질문은 다음 조건을 만족할 때 **정답으로 간주**된다:
- 위에서 설명한 방법으로 계산된 벡터에 가장 가까운 단어가, 질문에서 요구하는 **정확한 단어와 동일**할 것.
즉, **동의어(synonym)는 오답 처리**된다. 따라서 100% 정확도 달성은 사실상 불가능하다. 왜냐하면 현재 모델은 단어의 형태학적 정보(word morphology)에 대한 입력을 갖고 있지 않기 때문이다.
그러나 우리는 단어 벡터의 유용성이 특정 응용에서 이 정확도 지표와 **양의 상관관계(positively correlated)** 를 가질 것이라고 믿는다. 더 나아가, 특히 문법적 질문(syntactic questions)의 경우, 단어의 **구조적 정보(structural information)** 를 통합하면 추가적인 발전이 가능하다.
- <font color="#92d050">Semantic-Syntactic Word Relationship Test Set 제작</font>
<font color="#92d050">    - 의미적 질문 5 종류</font>
<font color="#92d050">    - 문법적 질문 9 종류</font>
<font color="#92d050">    - 정답은 “벡터 연산 결과의 가장 가까운 단어 = 목표 단어” 일 때만 인정</font>
<font color="#92d050">- 평가 기준: 코사인 유사도 기반 가장 가까운 단어 검색 정확도</font>

---

### 4.2 정확도 극대화 (Maximization of Accuracy)

우리는 **Google News 코퍼스**를 사용하여 단어 벡터를 학습했다. 이 코퍼스는 약 **60억(6B) 토큰**을 포함한다. 어휘 크기는 가장 빈도 높은 **100만 단어**로 제한하였다.
명확히, 우리는 **시간 제약이 있는 최적화 문제**에 직면해 있다. 왜냐하면 **더 많은 데이터**를 사용하거나 **더 높은 차원의 단어 벡터**를 사용할수록 정확도가 향상될 것이라 예상되기 때문이다.
빠른 시간 안에 가능한 한 좋은 결과를 얻기 위해, 우리는 먼저 **가장 빈도 높은 3만 단어(30k)** 로 어휘를 제한한 상태에서, 학습 데이터의 **부분 집합(subset)** 을 이용해 학습된 모델들을 평가했다. **CBOW 아키텍처**를 사용하여 단어 벡터의 **차원 수**와 **학습 데이터 양**을 다르게 설정한 결과는 표 2에 제시되어 있다.
<font color="#92d050">Google News 코퍼스(60억 단어, 어휘 100만 개)로 학습</font>

결과를 보면, 일정 시점을 지나면 **차원을 더 늘리거나 학습 데이터를 더 추가하는 것의 개선 효과가 점점 줄어드는(diminishing returns)** 것을 확인할 수 있다. 따라서 우리는 **단어 벡터 차원 수와 학습 데이터 양을 함께 늘려야** 한다.
이 관찰은 자명해 보일 수 있으나, 주목할 점은 현재 많은 연구들이 비교적 큰 데이터셋으로 학습을 수행하면서도, **단어 벡터 차원은 불충분하게 작은 값(예: 50~100)** 을 사용하는 경우가 많다는 사실이다.
식 (4)에 따르면, 학습 데이터 양을 두 배로 늘리는 것은 벡터 차원을 두 배로 늘리는 것과 **계산 복잡도 증가량이 거의 동일**하다.
<font color="#92d050">- 관찰 결과:</font>
<font color="#92d050">    - 데이터 크기와 벡터 차원을 함께 늘려야 성능 개선</font>
<font color="#92d050">    - 단일 요인만 늘릴 경우 효과는 한계</font>


![](20250925224224.png)
표2: CBOW 아키텍처를 사용한 제한된 어휘 조건에서의 의미-구문 단어 관계 테스트 세트 정확도

![](20250925224239.png)
표3: 동일한 데이터로 학습된 640차원 단어 벡터를 사용한 아키텍처 비교

표 2와 표 4에서 보고된 실험에서는, 우리는 **확률적 경사 하강법(Stochastic Gradient Descent, SGD)** 과 **역전파(backpropagation)** 를 사용하여 **3 epoch** 동안 학습을 진행했다. 초기 학습률은 **0.025**로 설정하고, 학습이 진행됨에 따라 선형적으로 감소시켜 마지막 epoch이 끝날 때 0에 도달하도록 했다.

---

### 4.3 모델 아키텍처 비교 (Comparison of Model Architectures)

먼저, 동일한 학습 데이터를 사용하고, 단어 벡터 차원을 **640**으로 고정하여 서로 다른 모델 아키텍처들을 비교하였다. 이후의 실험에서는 **새롭게 구축한 의미-구문 단어 관계 테스트 세트(Semantic-Syntactic Word Relationship test set)** 전체를 사용했으며, 이는 상위 3만 단어 어휘에 제한되지 않은(full vocabulary) 상태이다. 또한, 단어들 간의 구문적 유사성에 초점을 둔 [20]의 테스트 세트 결과도 포함하였다.

학습 데이터는 여러 **LDC 코퍼스**로 구성되어 있으며, [18]에서 자세히 설명되어 있다(3억 2천만 단어, 어휘 8만 2천 개). 우리는 이 데이터를 사용해 이전에 학습된 **순환 신경망 언어 모델(RNNLM)** 과 비교하였다. 해당 모델은 단일 CPU에서 약 **8주**가 걸려 학습되었다.

또한, 우리는 동일한 640개의 은닉 유닛을 갖는 **피드포워드 NNLM**을 DistBelief 병렬 학습 [6]을 사용해 학습하였다. 여기서는 과거 **8개 단어의 히스토리**를 입력으로 사용했으므로, 투영층의 크기는 640×8640 \times 8640×8이 되어 NNLM은 RNNLM보다 더 많은 파라미터를 가진다.

표 3에서 볼 수 있듯이, RNN(특히 [20]에서 사용된 모델)의 단어 벡터는 주로 **구문적 질문(syntactic questions)** 에서 좋은 성능을 보였다. 반면 NNLM 벡터는 RNN보다 확실히 더 좋은 성능을 보였는데, 이는 놀라운 결과가 아니다. 왜냐하면 RNNLM에서 단어 벡터는 비선형 은닉층과 직접 연결되기 때문이다.
<font color="#92d050">RNNLM: 구문적에는 강하지만 의미적에는 약함</font>
<font color="#92d050">NNLM: RNNLM보다 전반적으로 성능 우수</font>

**CBOW 아키텍처**는 NNLM보다 구문적 과제에서 더 좋은 성능을 보였고, 의미적 과제에서는 거의 동일한 수준이었다. 마지막으로, **Skip-gram 아키텍처**는 구문적 과제에서는 CBOW보다 약간 낮은 성능을 보였지만(그럼에도 NNLM보다는 우수), 의미적 과제에서는 모든 다른 모델보다 훨씬 뛰어난 성능을 보였다.
<font color="#92d050">CBOW: 구문적 관계에서 NNLM보다 더 좋고, 의미적 관계에서는 비슷</font>
<font color="#92d050">Skip-gram: 구문적 관계는 CBOW보다 약간 낮지만, 의미적 관계에서는 모든 모델 중 최고</font>

다음으로, 우리는 단일 CPU만을 사용해 학습한 모델들을 평가하고, 결과를 공개된 단어 벡터들과 비교하였다. 비교 결과는 표 4에 제시되어 있다. **CBOW 모델**은 Google News 데이터의 부분집합으로 약 **1일** 만에 학습되었으며, **Skip-gram 모델**은 학습에 약 **3일**이 걸렸다.

![](20250925224548.png)
표4: 공개된 단어 벡터들과 우리의 모델에서 학습된 단어 벡터를 의미-구문 단어 관계 테스트 세트(Semantic-Syntactic Word Relationship test set)에서 비교

![](20250925224612.png)
표5: 동일한 데이터로 3 epoch 동안 학습한 모델과 1 epoch 동안 학습한 모델의 비교

이후 보고된 실험에서는 단지 **1 epoch**만 사용했다(이때 학습률은 선형적으로 감소하여 학습 종료 시점에 0에 도달하도록 설정했다). 동일한 데이터를 **3 epoch 반복 학습**하는 것보다, **2배 더 많은 데이터를 1 epoch 학습**하는 것이 더 낫거나 최소한 비슷한 성능을 내며, 약간의 속도 향상도 제공한다는 사실을 표 5에서 확인할 수 있다.

---

### 4.4 대규모 병렬 학습 (Large Scale Parallel Training of Models)

앞서 언급했듯이, 우리는 다양한 모델을 **DistBelief**라는 분산 프레임워크에 구현하였다. 아래에서는 **Google News 60억(6B) 토큰 데이터셋**을 사용하여, **미니배치 비동기 경사 하강법(mini-batch asynchronous gradient descent)** 과 **Adagrad [7]** 라 불리는 적응형 학습률 기법으로 학습한 여러 모델의 결과를 보고한다.
학습 시에는 **50개에서 100개의 모델 복제본(replicas)** 을 사용하였다. CPU 코어 수는 추정치인데, 이는 데이터센터의 머신들이 다른 프로덕션 작업과 공유되기 때문에 사용량이 크게 변동할 수 있기 때문이다.
또한, 분산 프레임워크의 오버헤드(overhead) 때문에, **CBOW 모델과 Skip-gram 모델의 CPU 사용량은 단일 머신에서 구현했을 때보다 훨씬 비슷하게 나타난다**는 점에 주목할 필요가 있다.
결과는 **표 6(Table 6)** 에 보고되어 있다.
<font color="#92d050">DistBelief 분산 프레임워크로 50~100 replicas, 수백 개 CPU 코어 사용</font>


![](20250925225143.png)
표6: DistBelief 분산 프레임워크를 사용해 학습한 모델들의 비교

![](20250925225158.png)
표7: Microsoft Sentence Completion Challenge에서의 모델 비교 및 결합

---

### 4.5 Microsoft Research Sentence Completion Challenge

**Microsoft Sentence Completion Challenge**는 최근 언어 모델링과 기타 자연어 처리(NLP) 기법을 발전시키기 위한 과제로 소개되었다 [32]. 이 과제는 **총 1,040개의 문장**으로 구성되며, 각 문장에는 **한 단어가 비어 있고**, 주어진 다섯 개의 합리적인 후보 단어 중에서 문장의 나머지 부분과 가장 잘 어울리는 단어를 선택하는 것이 목표이다.

이 데이터셋에서는 이미 여러 기법들의 성능이 보고되었다. 여기에는 **N-gram 모델**, **LSA 기반 모델 [32]**, **로그-이중선형 모델(log-bilinear model) [24]**, 그리고 현재까지 **최첨단(state-of-the-art)** 성능을 기록한 **순환 신경망(RNN) 조합 모델**이 포함되며, 이 모델은 벤치마크에서 **정확도 55.4%** 를 달성하였다 [19].

우리는 이 과제에서 **Skip-gram 아키텍처**의 성능을 탐구하였다. 먼저, [32]에서 제공된 **5천만(50M) 단어**를 이용해 **640차원 모델**을 학습하였다. 그 다음, 테스트 세트의 각 문장에서 **빈칸 단어를 입력**으로 사용하고, 문장 내의 모든 주변 단어들을 예측한다. 문장의 **최종 점수**는 이러한 개별 예측 점수들의 합으로 계산된다. 이 문장 점수를 바탕으로 가장 가능성이 높은 문장을 선택한다.

이전 연구 결과와 우리의 새로운 결과를 요약한 내용은 **표 7(Table 7)** 에 제시되어 있다. Skip-gram 모델 자체만으로는 이 과제에서 LSA 유사도보다 더 나은 성능을 보이지는 못했지만, **RNNLM으로부터 얻은 점수와 상호 보완적**이라는 점이 확인되었다. 따라서 두 모델의 결과를 **가중 결합(weighted combination)** 하였을 때, 새로운 **최첨단 성능인 58.9% 정확도**(개발 세트에서는 59.2%, 테스트 세트에서는 58.7%)를 달성하였다.
<font color="#92d050">RNNLM + Skip-gram 가중 조합 → 58.9% 정확도 (기존 최고 55.4%를 능가, 새로운 SOTA)</font>

---

## 5. Examples of the Learned Relationships

표 8은 여러 가지 **단어 간 관계(relationships)** 를 보여준다. 우리는 앞에서 설명한 접근법을 따른다. 즉, 관계는 두 단어 벡터의 차이를 계산한 후, 그 결과를 또 다른 단어 벡터에 더하는 방식으로 정의된다.
예를 들어, `Paris−France+Italy=Rome` 와 같은 방식이다.
보시다시피, 정확도는 상당히 높은 편이지만, 여전히 개선의 여지가 많다. (참고로, 우리가 사용한 **정확히 일치하는 단어만을 정답으로 인정하는 정확도 측정 방식**에 따르면, 표 8의 결과는 약 60% 수준에 해당한다.)
우리는 **더 큰 데이터셋과 더 높은 차원의 벡터**로 학습된 단어 표현이 훨씬 더 좋은 성능을 보일 것이라고 믿으며, 이를 통해 **새로운 혁신적 응용**의 개발이 가능할 것이라 기대한다.
정확도를 향상시킬 또 다른 방법은 **하나의 예시가 아닌 여러 개의 예시**를 사용해 관계를 정의하는 것이다. 예를 들어, 관계 벡터를 만들 때 하나의 예시 대신 10개의 예시를 사용해 각각의 벡터를 평균하면, 우리의 최고 모델은 **의미-구문 테스트(semantic-syntactic test)** 에서 절대적으로 약 10%의 정확도 향상을 보였다.
<font color="#92d050">단일 예시 대신 여러 예시를 평균하면 정확도 약 10% 향상</font>

![](20250925225523.png)
표8: 단어 쌍 관계의 예시

또한, 이러한 벡터 연산을 **다른 과제**에 적용하는 것도 가능하다. 예를 들어, 단어 리스트의 평균 벡터를 계산한 뒤, 그와 가장 멀리 떨어진 단어 벡터를 찾음으로써, **리스트에서 어울리지 않는 단어를 선택하는 문제(out-of-the-list words)** 에서 좋은 정확도를 보였다. 이는 실제로 일부 **지능검사(human intelligence test)** 에서 자주 사용되는 문제 유형이다.
분명히, 이러한 기법을 활용하여 앞으로도 많은 발견들이 이루어질 수 있을 것이다.
<font color="#92d050">리스트에서 어울리지 않는 단어 찾기(out-of-list detection) 등 다른 과제에도 적용 가능.</font>

---
##  6. 결론

본 논문에서는 다양한 모델로부터 도출된 **단어 벡터 표현(word vector representations)** 의 품질을 **구문적(syntactic)** 및 **의미적(semantic) 언어 과제** 모음에서 평가하였다. 우리는 피드포워드(feedforward)와 순환(recurrent) 신경망 기반의 인기 있는 모델들에 비해, **매우 단순한 아키텍처**만으로도 **고품질 단어 벡터**를 학습할 수 있음을 확인하였다.
계산 복잡도가 훨씬 낮기 때문에, 훨씬 더 큰 데이터셋에서도 **고차원 단어 벡터를 정확하게 학습**할 수 있다. 특히, **DistBelief 분산 프레임워크**를 활용하면, 사실상 어휘 크기 제한 없이 **1조(trillion) 단어 규모의 코퍼스**로도 CBOW와 Skip-gram 모델을 학습하는 것이 가능할 것이다. 이는 이전에 발표된 유사 모델들의 최고 성과보다 **수십 배~수천 배 큰 규모**다.
<font color="#92d050">발견점:</font>
<font color="#92d050">- 기존 **NNLM, RNNLM**보다 훨씬 단순한 아키텍처(CBOW, Skip-gram)로도 **고품질 단어 벡터**를 학습할 수 있음.</font>
<font color="#92d050">- 계산 복잡도가 낮기 때문에 **대규모 데이터셋 + 고차원 벡터** 학습이 가능.</font>
<font color="#92d050">- Google의 **DistBelief 분산 프레임워크**를 사용하면 **수조 단어(trillion words)** 규모에서도 학습 가능 → 이전 연구보다 수십~수천 배 확장 가능.</font>

흥미로운 과제로는 최근 단어 벡터가 기존 **최첨단(state-of-the-art)** 방법들을 크게 능가한 **SemEval-2012 Task 2 [11]** 가 있다. 공개된 RNN 벡터와 다른 기법들을 결합해, 기존 최고 성과 대비 **스피어만 순위 상관계수(Spearman’s rank correlation)** 에서 50% 이상 개선을 달성하였다 [31].
신경망 기반 단어 벡터는 이전에도 **감성 분석(sentiment analysis) [12]**, **패러프레이즈(paraphrase) 탐지 [28]** 등 다양한 NLP 과제에 적용된 바 있으며, 본 논문에서 설명한 아키텍처는 이러한 응용에도 도움을 줄 수 있을 것으로 예상된다.
<font color="#92d050">응용 성과:</font>
<font color="#92d050">- **SemEval-2012 Task 2**에서 기존 SOTA 대비 **Spearman 상관계수 50% 이상 개선**.</font>
<font color="#92d050">- 신경망 기반 단어 벡터는 이미 **감성 분석(sentiment analysis)**, **패러프레이즈(paraphrase) 검출** 등 다양한 NLP 과제에 적용됨.</font>
<font color="#92d050">- 향후 더 많은 응용(예: 지식베이스 확장 및 검증, 기계 번역)에도 활용 가능</font>

현재 진행 중인 연구에서는, 단어 벡터가 **지식 베이스(Knowledge Base)에서 사실(facts)을 자동으로 확장**하거나, **기존 사실의 정확성을 검증**하는 데도 성공적으로 적용될 수 있음을 보여주고 있다. 또한, **기계 번역(machine translation)** 실험에서도 매우 유망한 결과가 나타나고 있다.
향후에는 본 논문의 기법을 **잠재 관계 분석(Latent Relational Analysis, LRA) [30]** 및 다른 기법들과 비교하는 것도 흥미로울 것이다.
우리는 이번에 제안한 **포괄적 테스트 세트**가 연구 공동체가 기존 단어 벡터 추정 기법을 개선하는 데 기여할 것이라 믿는다. 또한, **고품질 단어 벡터**는 앞으로의 **자연어 처리(NLP) 응용을 위한 핵심 빌딩 블록**이 될 것이라고 확신한다.
<font color="#92d050">미래 연구 방향:</font>
<font color="#92d050">- **Latent Relational Analysis (LRA)** 등 다른 기법과 비교 필요.</font>
<font color="#92d050">- 본 논문에서 제시한 **테스트 세트**는 단어 벡터 품질 개선에 중요한 벤치마크 역할을 할 것.</font>
<font color="#92d050">최종 결론:</font>
<font color="#92d050">- **고품질 단어 벡터**는 앞으로의 NLP 응용에 있어 **핵심 빌딩 블록**이 될 것이다.</font>

---

### 7. 후속 연구 (Follow-Up Work)

이 논문의 초기 버전이 작성된 후, 우리는 **연속형 Bag-of-Words(CBOW)** 와 **Skip-gram 아키텍처**를 모두 사용할 수 있는 **단일 머신 멀티스레드 C++ 코드**를 공개하였다.
이 코드의 학습 속도는 본 논문에서 앞서 보고한 것보다 훨씬 빠르며, 일반적인 하이퍼파라미터 설정에서 **시간당 수십억 단어(billions of words per hour)** 를 처리할 수 있다.
또한, 우리는 **1억 단어 이상(100 billion words)** 으로 학습한 결과물 중에서 **고유 명사(named entities)** 를 포함한 **140만 개 이상의 단어 벡터**를 공개하였다.
우리의 후속 연구 중 일부는 다가오는 **NIPS 2013 논문 [21]** 에서 발표될 예정이다.