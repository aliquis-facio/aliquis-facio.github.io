# Softmax

## 1. 기본 정의

**Softmax 함수**: 실수 벡터(예: 신경망의 출력값, logit)를 **전체 합이 1인 확률 분포(probability distribution)** 로 변환하는 함수이다.

“가장 큰 값을 1에 가깝게, 나머지는 0에 가깝게 바꿔주는 정규화(normalization) 함수”
**값의 상대적 크기 차이를 강조**

---

## 2. 수식 정의

입력 벡터: 
$$z = [z_1, z_2, ..., z_n]$$

Softmax 함수는 다음과 같이 정의된다:
$$\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}$$

| 기호               | 의미                   |
| ---------------- | -------------------- |
| $z_i$            | i번째 클래스의 점수(logit)   |
| $e^{z_i}$        | 지수 함수로 양수화           |
| $\sum_j e^{z_j}$ | 모든 클래스 점수의 합 (정규화 항) |

---

## 3. 수학적 성질

| 성질                               | 설명                    |
| -------------------------------- | --------------------- |
| $\sum_i \text{softmax}(z_i) = 1$ | 전체 합이 항상 1 → 확률 분포    |
| $0 < \text{softmax}(z_i) < 1$    | 모든 값이 0~1 사이          |
| 입력이 모두 같을 때                      | 모든 클래스 확률 = 1/n       |
| 값이 큰 쪽이 강조됨                      | exponential 함수의 특징 때문 |

---

## 4. 왜 지수(exponential)를 쓰는가?

지수를 사용하면 **값의 차이가 비선형적으로 확대**된다.
예를 들어:

```
z = [1, 2, 3]
→ e^z = [2.7, 7.4, 20.1]
```

이제 작은 차이(1→2→3)가 훨씬 뚜렷하게 구분된다.

이 덕분에 softmax는 모델이 "가장 가능성이 높은 클래스"에 더 강하게 집중하게 한다.

---

## 5. Softmax와 Cross-Entropy의 관계

Softmax는 보통 **Cross-Entropy Loss**와 함께 사용된다.  
둘을 합치면 다음처럼 해석:
$$L = -\sum_{i=1}^{n} y_i \log(\text{softmax}(z_i))$$
- $y_i$ = 실제 정답(one-hot 벡터)
- $\text{softmax}(z_i)$ = 예측 확률

모델이 예측한 확률이 정답 클래스에 가까워질수록 손실이 작아짐.

---

## 6. 모델에서의 사용 예시

|분야|사용 위치|역할|
|---|---|---|
|**분류(Classification)**|출력층|각 클래스 확률로 변환 (예: 고양이=0.9, 개=0.1)|
|**기계 번역(Decoder)**|다음 단어 확률 예측|(P(y_t|
|**Attention Mechanism**|가중치 계산|각 단어가 얼마나 중요한지 확률로 표현|
|**Reinforcement Learning**|정책 확률|행동(action)의 선택 확률화|

---

## 7. 주의할 점

1. **수치 안정성(Numerical Stability)**
	- 큰 값의 지수를 계산하면 overflow 발생 가능
	- 해결책:  
	    $$\text{softmax}(z_i) = \frac{e^{z_i - \max(z)}}{\sum_j e^{z_j - \max(z)}}$$
	    → 모든 입력에서 가장 큰 값을 빼줌 (결과 동일, overflow 방지)
2. **확률의 오용 주의**
	- softmax의 출력은 “상대적 확신도”일 뿐, “절대적인 확률” X