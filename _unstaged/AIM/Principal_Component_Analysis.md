# PCA(Principal Component Analysis, 주성분 분석)

## 1. 기본 정의

**PCA(주성분 분석, Principal Component Analysis)**: 고차원 데이터에서 **가장 중요한 방향(주성분, Principal Component)** 을 찾아 데이터를 **낮은 차원으로 투영(projection)** 하는 방법이다.

“데이터를 최대한 잃지 않으면서, 축을 새로 만들어 단순화하는 기법”

---

## 2. PCA의 핵심 아이디어

데이터가 **가장 많이 퍼진 방향(분산이 가장 큰 방향/상관관계가 많은 방향)** 을 찾아 그 축으로 회전(rotation)시켜 차원을 줄이면서도 데이터의 **분산(variance)** 을 최대한 유지한다.

“데이터의 본질적인 변화 방향”을 찾는 과정이다.

---

## 3. 수식 정의

PCA는 **데이터의 공분산 행렬(Covariance Matrix)** 를 이용

1. 데이터 중심화
	평균을 0으로 맞춤:
	$$X_c = X - \bar{X}$$
2. 공분산 행렬 계산
	$$\Sigma = \frac{1}{n} X_c^T X_c$$

3. 고유값 분해(Eigen Decomposition)
	$$\Sigma v_i = \lambda_i v_i$$

| 기호          | 의미                                         |
| ----------- | ------------------------------------------ |
| $v_i$       | i번째 **고유벡터(eigenvector)** → 주성분 방향         |
| $\lambda_i$ | i번째 **고유값(eigenvalue)** → 그 방향의 중요도(분산 크기) |
4. 주성분 선택
	1. 고유값이 큰 순서로 k개 선택 (데이터의 분산 대부분 설명)
	2. 데이터를 새 축으로 투영:
	    $$Z = X_c V_k$$
	    여기서 $V_k$는 상위 k개의 고유벡터

---

## 4. 차원 축소 후 분산 비율 (Explained Variance Ratio)

각 주성분이 데이터의 정보를 얼마나 설명하는지를 나타낸다:
$$\text{Explained Variance Ratio} = \frac{\lambda_i}{\sum_j \lambda_j}$$

|주성분|설명된 분산 비율|
|---|---|
|PC1|70%|
|PC2|20%|
|PC3|5%|
|…|…|

상위 2개 주성분만으로 전체 데이터의 90% 이상 설명 가능하다

---

## 5. PCA의 장점 & 단점

| 장점               | 단점                               |
| ---------------- | -------------------------------- |
| 고차원 데이터 시각화 가능   | 비선형 관계는 반영 못함                    |
| 노이즈 제거, 계산 효율 향상 | 해석이 직관적이지 않음 (새 축이 의미 없는 조합일 수도) |
| 과적합 방지           | 스케일에 민감 (표준화 필요)                 |

---

## 6. PCA의 응용 분야

|분야|예시|
|---|---|
|**데이터 전처리**|고차원 → 저차원 (특징 축소)|
|**시각화**|100차원 데이터를 2D로 시각화|
|**노이즈 제거**|잡음이 큰 축 제거|
|**압축**|이미지/음성 데이터를 주요 성분으로 압축|
|**전처리 단계**|LDA, SVM, K-Means 전에 차원 축소용|
