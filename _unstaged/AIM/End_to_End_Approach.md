# End-to-End Approach

중간 단계를 최소화하고 입력에서 출력까지 전체를 하나의 모델이 직접 학습하는 방식

## 1. 기본 정의

**엔드 투 엔드(End-to-End) 접근법**: “입력(Input)”으로부터 “출력(Output)”까지의 전체 과정을 **하나의 통합된 모델**이 직접 학습하는 방식이다.
**사람이 설계한 중간 규칙(rule-based step)** 나 **수작업으로 만든 피처(feature engineering)** 없이, 모델이 **데이터로부터 직접 최적의 표현과 변환 과정을 학습**한다.

## 2. 전통적 접근과의 비교

|구분|전통적(비엔드투엔드) 접근|엔드투엔드 접근|
|---|---|---|
|**파이프라인 구조**|여러 개의 독립된 단계로 구성 (모듈 간 연결 필요)|하나의 모델이 전체 과정 수행|
|**중간 피처**|사람이 직접 설계 (feature engineering)|모델이 스스로 학습 (representation learning)|
|**학습 대상**|각 단계별 개별 학습|입력→출력 전체를 통합 학습|
|**예시**|음성 → 음소 → 단어 → 문장|음성 → 바로 문장|
|**장점**|해석이 쉬움, 디버깅 용이|단순, 성능 향상, 데이터 효율|
|**단점**|오류 전파(error propagation), 복잡한 조정|많은 데이터 필요, 내부 해석 어려움|


## 3. 수식적 표현

엔드투엔드 학습은 다음과 같이 표현할 수 있습니다:
$$y = f_\theta(x)$$
- $x$: 입력 (예: 음성 신호, 문장, 이미지 등)
- $y$: 출력 (예: 번역문, 텍스트 라벨 등)
- $f_\theta$​: 파라미터 $θ$를 가진 신경망 모델
- 학습 목표:
    $\min_\theta \mathcal{L}(f_\theta(x), y)$

즉, 입력과 출력 간의 오차를 최소화하도록 **모델 전체를 통합적으로 학습**

## 4. 장점

- **간결함 (Simplicity)**: 모든 단계가 하나의 모델 안에 통합 → 시스템 구조 단순
- **최적화 효율성 (Optimization Efficiency)**: 전체 파이프라인을 **하나의 손실 함수(loss function)** 로 직접 최적화 가능
- **자동 표현 학습 (Representation Learning)**: 모델이 데이터로부터 스스로 **의미 있는 피처(feature)** 를 학습
- **성능 향상 (Performance Boost)**: 사람의 개입을 줄이고, 대규모 데이터로 학습 시 더 나은 결과

## 5. 단점

- **대량의 데이터 필요**: 중간 지식 없이 학습하려면 방대한 데이터셋 필요
- **해석 불가 (Black Box)**: 모델 내부가 복잡해, 어떤 이유로 특정 출력을 내는지 분석 어려움
- **오류 원인 추적 어려움**: 한 모델이 전체를 처리하므로 문제 발생 시 위치 파악이 어려움

## 6. Seq2Seq 모델의 엔드투엔드 구조

- **기존 SMT 방식:**
	`문장 분석 → 구문 트리 생성 → 단어 정렬 → 번역 규칙 적용 → 문장 생성`
- **Seq2Seq (엔드투엔드):**
	`입력 문장 → LSTM Encoder → 벡터 표현 → LSTM Decoder → 출력 문장`

> 중간 단계 없이, 입력 문장에서 출력 문장으로 직접 매핑(mapping)