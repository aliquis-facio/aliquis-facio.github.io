# 장기 의존성(Long-term Dependency)

## 1. 기본 정의

**장기 의존성(Long-term dependency)**: **문장이나 시퀀스에서 멀리 떨어진 정보가 나중의 결과에 영향을 미치는 현상**
문장 초반의 정보가 문장 끝 부분의 해석에 **중요한 영향을 미칠 때**, 모델이 **그 오래된 정보를 기억하고 활용해야 하는 상황**입니다.

---

## 2. 예시로 이해하기

### 2.1. 간단한 문장 예시

```
“그녀는 너무 피곤해서 결국 __ 잠이 들었다.”
```

- 빈칸에 들어갈 단어 “잠이 들었다”는 문장 초반의 “피곤해서”와 관련 있음.
	- 이건 **짧은 의존성(short-term dependency)**

---

### 2.2. 장기 의존성 예시

```
“나는 [10문장 생략] ... 왜냐하면 그녀가 피곤했기 때문이다.”
```

- 문장 끝의 “피곤했기 때문이다”는 맨 앞의 “그녀가 지친 하루를 보냈다”와 연결됨.
	- **10문장 전의 정보가 현재 해석에 필요함**
	- 이것이 **장기 의존성(long-term dependency)** 문제이다.

---

## 3. [RNN](AIM/Recurrent_Neural_Network.md)에서의 장기 의존성 문제

### 3.1. RNN 기본 구조

RNN은 이전 단계의 은닉 상태를 다음 입력과 함께 사용:
$$h_t = f(Wx_t + Uh_{t-1} + b)$$

하지만, 시간이 지날수록(시퀀스가 길어질수록) 이전 정보의 영향이 점점 약해집니다.

이걸 **“기울기 소실(vanishing gradient)” 문제**라고 합니다.

---

## 4. RNN이 겪는 문제

|문제|설명|
|---|---|
|**기울기 소실 (Vanishing Gradient)**|역전파 중에 과거 시점으로 갈수록 기울기가 0에 수렴 → 오래된 정보가 학습되지 않음|
|**기울기 폭발 (Exploding Gradient)**|반대로 기울기가 너무 커져 학습 불안정|
|**메모리 한계**|오래된 정보가 은닉 상태에서 완전히 사라짐|
|**의미 단절**|문맥 상 멀리 떨어진 단어의 관계를 인식하지 못함|

결과적으로, RNN은 **짧은 의존성(short-term)** 은 잘 처리하지만 **긴 문맥(long-term dependency)** 은 거의 기억하지 못한다.

---

## 5. [LSTM](AIM/Long_Short_Term_Memory.md)이 등장한 이유

LSTM(Long Short-Term Memory)은 바로 이 “장기 의존성 문제”를 해결하기 위해 설계된 RNN 구조이다.

### 5.1. LSTM의 핵심 아이디어

- **“정보를 잊을지(forget), 기억할지(remember), 출력할지(output)”** 스스로 조절
- “Cell state(셀 상태)”를 통해 오래된 정보를 보존 → 과거 수백 step 전의 정보도 유지 가능

그래서 이름이 “**Long Short-Term Memory (긴 단기 기억)**” 즉, _짧은 기간뿐 아니라 긴 기간의 정보도 유지할 수 있다_ 는 뜻이다.

---

## 6. 시각적 예시 (텍스트에서의 장기 의존성)

```
문장: The animal that I saw in the zoo was a lion.
```

|단어|의미상 연결|
|---|---|
|The animal|↔ was a lion (주어-서술 관계, 문장 끝까지 영향)|
|that I saw in the zoo|(중간에 끼어드는 절) → noise 역할|

> “animal”과 “was” 사이에 5단어 이상 떨어져 있지만 문법적으로는 연결되어 있음 → **장기 의존성**

---

## 7. 장기 의존성의 일반적 예시들

|분야|예시|
|---|---|
|**자연어 처리**|문맥 기억 (대화, 문장 구조, 대명사 참조 등)|
|**음성 인식**|말의 맥락 유지 (“안녕” 후 “하세요”)|
|**기계 번역**|앞 문장의 주어/시제/의미가 뒤에 영향을 줌|
|**시계열 예측**|주가 예측에서 수개월 전 패턴이 영향을 미침|
